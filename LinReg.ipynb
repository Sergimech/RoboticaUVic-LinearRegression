{
 "metadata": {
  "name": "",
  "signature": "sha256:3dcf913c8b507d2cca80a33879989b78fc64c25140319c3152ca1759c2203665"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1 Download data DONE   \n",
      "\u03a92 Load the data (using numpy.loadtxt) and separate the last column (target value, MEDV). \n",
      "Compute the average of the target value and the MSE obtained using it as a constant prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import library\n",
      "import numpy as np\n",
      "from scipy.linalg import lstsq as theta_calc\n",
      "from scipy.linalg import lstsq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Decl MSE Function\n",
      "def MSE(y,X,theta):\t\n",
      "\tMSE = float(sum((y-dot(X, theta))**2) / len(y)) \n",
      "\treturn MSE  \n",
      "data = np.loadtxt('/data.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Slicing\n",
      "y = data[:,-1]  #Outputs (labels) / The last column which is price save in 'y' / MEDVMedian value of owner-occupied homes in $1000's\n",
      "print '\\nHave a dataset of %d data points' % len(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Have a dataset of 506 data points\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shortcuts\n",
      "dot = np.dot\t#Multiplier\n",
      "inv = np.linalg.inv #Inversor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dummy features / Create a ones vector with the y lenght \n",
      "bias = np.ones((len(y),1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fitting the parameters: theta = (X'*X)^-1*X'*y\n",
      "# Computing the average of the target value\n",
      "theta = theta_calc(bias,y)[0]\n",
      "print 'The average of the target value is:' , theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The average of the target value is: [ 22.53280632]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# MSE = (1/N)*sum((y-X*theta)^2) // Computing MSE obtained using it as a constant prediction\n",
      "print '\\nMSE as a constant prediction: ' , MSE(y,bias,theta)\n",
      "print ('------------------------------------------------------------------------------------------------')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "MSE as a constant prediction:  84.4195561562\n",
        "------------------------------------------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Data.File and named in data\n",
      "data = np.loadtxt('/data.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u03a93 Split the data in two parts (50%-50%) for training and testing (first half for training, \n",
      "second half for testing). Train a linear regressor model for each variable individually \n",
      "(plus a bias term) and compute the MSE on the training and the testing set. Which variable \n",
      "is the most informative? which one makes the model generalize better? and worse? Compute the\n",
      "coefficient of determination (R^2) for the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Take the half lenght about las column\n",
      "half = len(y)/2 \n",
      "print '\\nTake a half of dataset, %d data points' % half"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Take a half of dataset, 253 data points\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#For training \n",
      "data_train = data[:half,:-1]\t#Training features, take 13 columns and the first half of total rows\n",
      "output_train = data[:half,-1]\t#Training labels, take the first half of target value column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#For testing\n",
      "data_test = data[half:,:-1]\t\t#Testing features, take 13 columns and the second half of total rows\n",
      "output_test = data[half:,-1]\t#Testing labels, take the second half of target value column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '\\nTraining and testing with each variable individually...'\n",
      "#Create lists\n",
      "theta_list=[]\n",
      "mse_train_list=[]\n",
      "mse_test_list=[]\n",
      "#We catch each column and create a matrix with the first column of some and another column with data from each label\n",
      "#We make lists about Fse for each label\n",
      "for i in range(0,len(data[0])-1): #Columns FOR until 0 to 13\n",
      "    train_features = np.hstack((bias[:half], data_train[:,i].reshape(half,1))) #Stack a feature array (vector) with Bias term (training)\n",
      "    test_features = np.hstack((bias[:half], data_test[:,i].reshape(half,1))) #Stack a feature array (vector) with Bias term (testing)\n",
      "    theta_list.append(theta_calc(train_features,output_train)[0]) #Save every theta for each different feature\n",
      "    mse_train_list.append(MSE(output_train,train_features,theta_list[i])) #Save every MSE for each theta to a list (training)\n",
      "    mse_test_list.append(MSE(output_test,test_features,theta_list[i])) #Save every MSE for each theta to a list (testing)\n",
      "\n",
      "# Answering the Q3's questions\n",
      "print '\\nThe variable %d is the most informative, MSE:' % mse_train_list.index(min(mse_train_list)), min(mse_train_list)\n",
      "print '\\nThe most generalizable variable is the number', mse_test_list.index(min(mse_test_list)),', MSE:', min(mse_test_list)\n",
      "print '\\nThe worst generalizable variable is the number', mse_test_list.index(max(mse_test_list)),', MSE:', max(mse_test_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training and testing with each variable individually...\n",
        "\n",
        "The variable 5 is the most informative, MSE: 15.9351479206\n",
        "\n",
        "The most generalizable variable is the number 12 , MSE: 43.3437235953\n",
        "\n",
        "The worst generalizable variable is the number 0 , MSE: 873.516873624\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Computing the coefficient of determination\n",
      "mean = sum(output_test) / half\t\t#mean = (1/N)*sum(y)\n",
      "VAR = sum((mean-output_test)**2) / half\t\t#VAR = (1/N)*sum((mean-y)^2)\n",
      "FUV = mse_test_list/VAR\t\t#Fraction Variance Unexplained (FUV) -> FUV = MSE/VAR\n",
      "Coeff_Det_Q3 = 1 - FUV\t\t#Coefficient of determination -> R^2 = 1-FUV\n",
      "print '\\n\\nThe most useful variable is %d giving a coefficient of determination (R^2) of: ' % Coeff_Det_Q3.argmax(axis=0), max(Coeff_Det_Q3)\n",
      "print ('------------------------------------------------------------------------------------------------')\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "The most useful variable is 12 giving a coefficient of determination (R^2) of:  0.535370828363\n",
        "------------------------------------------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4 Now train a model with all the variables plus a bias term. What is the performance in the test set? Try removing the worst-performing variable you found in step 3, and run again the experiment. What happened"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Removing the worst-performing variable in the features array\n",
      "data_train_Q4 = np.delete(data_train,Coeff_Det_Q3.argmin(axis=0),axis=1)\n",
      "data_test_Q4 = np.delete(data_test,Coeff_Det_Q3.argmin(axis=0),axis=1)\n",
      "\n",
      "# Starting over the experiment\n",
      "train_features = np.hstack((bias[:half], data_train_Q4.reshape(half,data_train_Q4.shape[1])))\t#Stack all features with Bias term (training)\n",
      "test_features = np.hstack((bias[:half], data_test_Q4.reshape(half,data_test_Q4.shape[1]))) #Stack all features with Bias term (testing)\n",
      "theta = theta_calc(train_features,output_train)[0]\t#Calculating the model (theta)\n",
      "MSE_train = MSE(output_train,train_features,theta)\t#Calculating the error (training)\n",
      "MSE_test = MSE(output_test,test_features,theta)\t\t\t#Calculating the error (testing)\n",
      "\n",
      "mean = sum(output_test) / half\t\t#mean = (1/N)*sum(y)\n",
      "VAR = sum((mean-output_test)**2) / half\t\t#VAR = (1/N)*sum((mean-y)^2)\n",
      "FUV = MSE_test / VAR\t\t#Fraction of Variance Unexplained (FUV) -> FUV = MSE/VAR\n",
      "Coeff_Det_Q4 = 1 - FUV\t\t#Coefficient of determination -> R^2 = 1-FUV\n",
      "\n",
      "print '\\nThe resulting MSE in the training phase is:' , MSE_train\n",
      "print '\\nThe resulting MSE in the testing phase is:' , MSE_test\n",
      "print '\\nThe resulting Coefficient of determination is:' , Coeff_Det_Q4\n",
      "\n",
      "print '\\nThe model is better.'\n",
      "print ('------------------------------------------------------------------------------------------------')\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The resulting MSE in the training phase is: 10.2162833878\n",
        "\n",
        "The resulting MSE in the testing phase is: 50.4903768903\n",
        "\n",
        "The resulting Coefficient of determination is: 0.458761268201\n",
        "\n",
        "The model is better.\n",
        "------------------------------------------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q5 We can give more capacity to a linear regression model by using basis functions (Bishop, sec. 3.1). In short, we can apply non-linear transformations to the input variables to extend the feature vector.\n",
      "Here we will try a polynomial function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PIL import Image\n",
      "im = Image.open(\"/polyy.jpeg\")\n",
      "im.rotate(0).show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repeat step 3 but adding, one by one, all polynomials up to degree 4. What are the effects of adding more capacity to the model?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#No-lineal Regresion \n",
      "\n",
      "#Data arrays\n",
      "#MSE polynomial\n",
      "\n",
      "training_theta_nonlineal_X = []\n",
      "training_MSE_nonlineal_X = []\n",
      "testing_MSE_nonlineal_X = []\n",
      "\n",
      "training_theta_nonlineal_X2 = []\n",
      "training_MSE_nonlineal_X2 = []\n",
      "testing_MSE_nonlineal_X2 = []\n",
      "\n",
      "training_theta_nonlineal_X3 = []\n",
      "training_MSE_nonlineal_X3 = []\n",
      "testing_MSE_nonlineal_X3 = []\n",
      "\n",
      "training_theta_nonlineal_X4 = []\n",
      "training_MSE_nonlineal_X4 = []\n",
      "testing_MSE_nonlineal_X4 = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "#Training\n",
      "    x1_training = data_train[:,i].reshape(half,1)\n",
      "    x2_training = data_train[:,i].reshape(half,1) ** 2\n",
      "    x3_training = data_train[:,i].reshape(half,1) ** 3\n",
      "    x4_training = data_train[:,i].reshape(half,1) ** 4\n",
      "    \n",
      "    training_data_Q5_X = np.hstack((bias[:half], x1_training))\n",
      "    training_data_Q5_X2 = np.hstack((bias[:half], x1_training, x2_training))\n",
      "    training_data_Q5_X3 = np.hstack((bias[:half], x1_training, x2_training, x3_training))\n",
      "    training_data_Q5_X4 = np.hstack((bias[:half], x1_training, x2_training, x3_training, x4_training))\n",
      "    \n",
      "    training_theta_nonlineal_X.append(lstsq(training_data_Q5_X, output_train)[0])\n",
      "    training_theta_nonlineal_X2.append(lstsq(training_data_Q5_X2, output_train)[0])\n",
      "    training_theta_nonlineal_X3.append(lstsq(training_data_Q5_X3, output_train)[0])\n",
      "    training_theta_nonlineal_X4.append(lstsq(training_data_Q5_X4, output_train)[0])\n",
      "    \n",
      "\n",
      "#Testing\n",
      "    x1_testing = data_test[:,i].reshape(half,1)\n",
      "    x2_testing = data_test[:,i].reshape(half,1) ** 2\n",
      "    x3_testing = data_test[:,i].reshape(half,1) ** 3\n",
      "    x4_testing = data_test[:,i].reshape(half,1) ** 4\n",
      "    \n",
      "    testing_data_Q5_X = np.hstack((bias[:half], x1_testing))\n",
      "    testing_data_Q5_X2 = np.hstack((bias[:half], x1_testing, x2_testing))\n",
      "    testing_data_Q5_X3 = np.hstack((bias[:half], x1_testing, x2_testing, x3_testing))\n",
      "    testing_data_Q5_X4 = np.hstack((bias[:half], x1_testing, x2_testing, x3_testing, x4_testing))\n",
      "    \n",
      "    training_MSE_nonlineal_X.append(MSE(output_train, training_data_Q5_X, training_theta_nonlineal_X[i]))\n",
      "    training_MSE_nonlineal_X2.append(MSE(output_train, training_data_Q5_X2, training_theta_nonlineal_X2[i]))\n",
      "    training_MSE_nonlineal_X3.append(MSE(output_train, training_data_Q5_X3, training_theta_nonlineal_X3[i]))\n",
      "    training_MSE_nonlineal_X4.append(MSE(output_train, training_data_Q5_X4, training_theta_nonlineal_X4[i]))\n",
      "    \n",
      "    testing_MSE_nonlineal_X.append(MSE(output_test, testing_data_Q5_X, training_theta_nonlineal_X[i]))\n",
      "    testing_MSE_nonlineal_X2.append(MSE(output_test, testing_data_Q5_X2, training_theta_nonlineal_X2[i]))\n",
      "    testing_MSE_nonlineal_X3.append(MSE(output_test, testing_data_Q5_X3, training_theta_nonlineal_X3[i]))\n",
      "    testing_MSE_nonlineal_X4.append(MSE(output_test, testing_data_Q5_X4, training_theta_nonlineal_X4[i]))\n",
      "    \n",
      "#Coefficient R\u00b2 nolineal\n",
      "\n",
      "#Mean = (1/N)*Sum(y)\n",
      "mean_no_lineal = sum(output_test)/len(output_test)\n",
      "#Var = (1/N)*Sum((Mean-y)\u00b2)\n",
      "var_no_lineal = sum((mean-output_test)**2)/len(output_test)\n",
      "#FVU = MSE/Var\n",
      "fvu_nonlineal_X = testing_MSE_nonlineal_X/VAR\n",
      "fvu_nonlineal_X2 = testing_MSE_nonlineal_X2/VAR\n",
      "fvu_nonlineal_X3 = testing_MSE_nonlineal_X3/VAR\n",
      "fvu_nonlineal_X4 = testing_MSE_nonlineal_X4/VAR\n",
      "#R2 = 1-FVU\n",
      "R2_no_lineal_X = 1-fvu_nonlineal_X\n",
      "R2_no_lineal_X2 = 1-fvu_nonlineal_X2\n",
      "R2_no_lineal_X3 = 1-fvu_nonlineal_X3\n",
      "R2_no_lineal_X4 = 1-fvu_nonlineal_X4\n",
      "\n",
      "#Printing results\n",
      "\n",
      "from tabulate import tabulate    \n",
      "\n",
      "print '----------------'\n",
      "print 'Printing results'\n",
      "print '----------------------------------------------------------------------------'\n",
      "print tabulate([['Train min. MSE no lineal', min(training_MSE_nonlineal_X), min(training_MSE_nonlineal_X2), min(training_MSE_nonlineal_X3), min(training_MSE_nonlineal_X4)], \n",
      "                ['Test min. MSE no lineal', min(testing_MSE_nonlineal_X), min(testing_MSE_nonlineal_X2), min(testing_MSE_nonlineal_X3), min(testing_MSE_nonlineal_X4)], \n",
      "                ['Test max. MSE no lineal', max(testing_MSE_nonlineal_X), max(testing_MSE_nonlineal_X2), max(testing_MSE_nonlineal_X3), max(testing_MSE_nonlineal_X4)],\n",
      "                ['Coefficient R2 nolineal', min(R2_no_lineal_X), min(R2_no_lineal_X2), min(R2_no_lineal_X3), min(R2_no_lineal_X4)]], \n",
      "               headers=['Polynomials', 'x1', 'x2', 'x3', 'x4'])\n",
      "print '----------------------------------------------------------------------------'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------\n",
        "Printing results\n",
        "----------------------------------------------------------------------------\n",
        "Polynomials                      x1           x2            x3            x4\n",
        "------------------------  ---------  -----------  ------------  ------------\n",
        "Train min. MSE no lineal   15.9351       14.4471  13.8041       13.3226\n",
        "Test min. MSE no lineal    43.3437       43.3093  42.444        40.9974\n",
        "Test max. MSE no lineal   873.517    125945        4.86329e+09   5.00716e+13\n",
        "Coefficient R2 nolineal    -8.36379   -1349.08    -5.21328e+07  -5.36749e+11\n",
        "----------------------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we look at the MSE we can see that it rises in value when we ascend in degree of polynomial, but when we observe the coefficient of determination, the opposite occurs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Gradient Descent\n",
      "\n",
      "As we have seen, overfitting is a problem that arises when we try to\n",
      "have more powerful methods, able to better adapt to the data. In order\n",
      "to reduce overfitting, we can **regularize** our model, but then we do\n",
      "not have a closed form solution and must resort to optimization. First\n",
      "we will use *Gradient Descent*, which is a widely used optimization\n",
      "algorithm. Here is a simple implementation in pseudo-code:\n",
      "\n",
      "\n",
      "0. Function Gradient_Descent\n",
      "1.   Initialize theta(0) at random\n",
      "2.   t=0, maxit=100, step=1e-6, loss=zeros(maxit)\n",
      "3.   loss(0) = f(theta)\n",
      "4.   do\n",
      "5.      t=t+1\n",
      "6.      theta(t) = theta(t-1) - step * f'(theta(t-1))\n",
      "7.      loss(t) = f(theta)\n",
      "8.   While t<maxit\n",
      "9.   return theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
